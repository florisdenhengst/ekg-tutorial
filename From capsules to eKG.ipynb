{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa4b910c",
   "metadata": {},
   "source": [
    "# From capsule to eKG\n",
    "\n",
    "This notebook has the minimum sample code to convert a series of 'capsules' representing an interaction into its graph representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997b3ea0",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "IN THIS SCENARIO, A MAN, CARL, HAS TO TAKE HIS MEDICATION PILLS BUT CANNOT FIND THEM.\n",
    "THE AGENT FINDS THEM THROUGH OBJECT RECOGNITION AND COMMUNICATES THIS TO CARL. \n",
    "CARL THEN FIND THE PILLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68caba76",
   "metadata": {},
   "source": [
    "## Prerequisites \n",
    "1. Download [GraphDB](http://graphdb.ontotext.com/)\n",
    "2. Launch it\n",
    "3. Create a repository, you can use [this configuration](https://github.com/leolani/cltl-knowledgerepresentation/blob/main/src/cltl/brain/ontologies/BASIC-REPOSITORY-CONFIG-GRAPHDB.ttl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8861365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "from datetime import date, datetime\n",
    "from pathlib import Path\n",
    "from random import getrandbits\n",
    "\n",
    "import requests\n",
    "from cltl.brain.long_term_memory import LongTermMemory\n",
    "from cltl.brain.utils.helper_functions import brain_response_to_json\n",
    "from cltl.commons.discrete import UtteranceType\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15feff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define contextual features\n",
    "context_id = getrandbits(8)\n",
    "place_id = getrandbits(8)\n",
    "location = requests.get(\"https://ipinfo.io\").json()\n",
    "start_date = date(2021, 3, 12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e434d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "carl_scenario = (\n",
    "    {\"context_id\": context_id,\n",
    "     \"date\": start_date,\n",
    "     \"place\": \"Carl's room\",\n",
    "     \"place_id\": place_id,\n",
    "     \"country\": location['country'],\n",
    "     \"region\": location['region'],\n",
    "     \"city\": location['city']},\n",
    "    [{  # CARL SAYS CANNOT SEE HIS PILLS\n",
    "        \"chat\": 1,\n",
    "        \"turn\": 1,\n",
    "        \"author\": {\"label\": \"carl\", \"type\": [\"person\"],\n",
    "                   'uri': \"http://cltl.nl/leolani/friends/carl-1\"},\n",
    "        \"utterance\": \"I need to take my pills, but I cannot find them.\",\n",
    "        \"utterance_type\": UtteranceType.STATEMENT,\n",
    "        \"position\": \"0-25\",\n",
    "        \"subject\": {\"label\": \"carl\", \"type\": [\"person\"],\n",
    "                    'uri': \"http://cltl.nl/leolani/world/carl-1\"},\n",
    "        \"predicate\": {\"label\": \"see\", \"uri\": \"http://cltl.nl/leolani/n2mu/see\"},\n",
    "        \"object\": {\"label\": \"pills\", \"type\": [\"object\", \"medicine\"],\n",
    "                   'uri': \"http://cltl.nl/leolani/world/pills\"},\n",
    "        \"perspective\": {\"certainty\": 1, \"polarity\": -1, \"sentiment\": -1},\n",
    "        \"timestamp\": datetime.combine(start_date, datetime.now().time()),\n",
    "        \"context_id\": context_id\n",
    "    }, {  # THE AGENT USES ITS CAMERA TO LOOK FOR THEM. SEES A CHAIR\n",
    "        \"visual\": 1,\n",
    "        \"detection\": 1,\n",
    "        \"source\": {\"label\": \"front-camera\", \"type\": [\"sensor\"],\n",
    "                   'uri': \"http://cltl.nl/leolani/inputs/front-camera\"},\n",
    "        \"image\": None,\n",
    "        \"utterance_type\": UtteranceType.EXPERIENCE,\n",
    "        \"region\": [752, 46, 1148, 716],\n",
    "        \"item\": {'label': 'chair 1', 'type': ['chair'], 'id': 1,\n",
    "                 'uri': \"http://cltl.nl/leolani/world/chair-1\"},\n",
    "        'confidence': 0.68,\n",
    "        \"timestamp\": datetime.combine(start_date, datetime.now().time()),\n",
    "        \"context_id\": context_id\n",
    "    }, {  # THE AGENT USES ITS CAMERA TO LOOK FOR THEM. SEES A TABLE\n",
    "        \"visual\": 1,\n",
    "        \"detection\": 2,\n",
    "        \"source\": {\"label\": \"front-camera\", \"type\": [\"sensor\"],\n",
    "                   'uri': \"http://cltl.nl/leolani/inputs/front-camera\"},\n",
    "        \"image\": None,\n",
    "        \"utterance_type\": UtteranceType.EXPERIENCE,\n",
    "        \"region\": [752, 86, 1148, 816],\n",
    "        \"item\": {'label': 'table 1', 'type': ['table'], 'id': 1,\n",
    "                 'uri': \"http://cltl.nl/leolani/world/table-1\"},\n",
    "        'confidence': 0.68,\n",
    "        \"timestamp\": datetime.combine(start_date, datetime.now().time()),\n",
    "        \"context_id\": context_id\n",
    "    }, {  # THE AGENT USES ITS CAMERA TO LOOK FOR THEM. SEES CARL\n",
    "        \"visual\": 1,\n",
    "        \"detection\": 3,\n",
    "        \"source\": {\"label\": \"front-camera\", \"type\": [\"sensor\"],\n",
    "                   'uri': \"http://cltl.nl/leolani/inputs/front-camera\"},\n",
    "        \"image\": None,\n",
    "        \"utterance_type\": UtteranceType.EXPERIENCE,\n",
    "        \"region\": [752, 46, 1700, 716],\n",
    "        \"item\": {'label': 'carl', 'type': ['person'], 'id': None,\n",
    "                 'uri': \"http://cltl.nl/leolani/world/carl-1\"},\n",
    "        'confidence': 0.94,\n",
    "        \"timestamp\": datetime.combine(start_date, datetime.now().time()),\n",
    "        \"context_id\": context_id\n",
    "    }, {  # THE AGENT USES ITS CAMERA TO LOOK FOR THEM. SEES PILLBOX\n",
    "        \"visual\": 1,\n",
    "        \"detection\": 4,\n",
    "        \"source\": {\"label\": \"front-camera\", \"type\": [\"sensor\"],\n",
    "                   'uri': \"http://cltl.nl/leolani/inputs/front-camera\"},\n",
    "        \"image\": None,\n",
    "        \"utterance_type\": UtteranceType.EXPERIENCE,\n",
    "        \"region\": [752, 46, 1148, 716],\n",
    "        \"item\": {'label': 'pillbox 1', 'type': ['pillbox'], 'id': 1,\n",
    "                 'uri': \"http://cltl.nl/leolani/world/pillbox-1\"},\n",
    "        'confidence': 0.92,\n",
    "        \"timestamp\": datetime.combine(start_date, datetime.now().time()),\n",
    "        \"context_id\": context_id\n",
    "    }, {  # THE AGENT SAYS THE PILLS ARE UNDER THE TABLE\n",
    "        \"chat\": 1,\n",
    "        \"turn\": 2,\n",
    "        \"author\": {\"label\": \"leolani\", \"type\": [\"robot\"],\n",
    "                   'uri': \"http://cltl.nl/leolani/friends/leolani\"},\n",
    "        \"utterance\": \"They are under the table.\",\n",
    "        \"utterance_type\": UtteranceType.STATEMENT,\n",
    "        \"position\": \"0-25\",\n",
    "        \"subject\": {\"label\": \"pills\", \"type\": [\"object\"], 'uri': \"http://cltl.nl/leolani/world/pills\"},\n",
    "        \"predicate\": {\"label\": \"located under\", \"uri\": \"http://cltl.nl/leolani/n2mu/located-under\"},\n",
    "        \"object\": {\"label\": \"table\", \"type\": [\"object\"], 'uri': \"http://cltl.nl/leolani/world/table\"},\n",
    "        \"perspective\": {\"certainty\": 1, \"polarity\": 1, \"sentiment\": 0},\n",
    "        \"timestamp\": datetime.combine(start_date, datetime.now().time()),\n",
    "        \"context_id\": context_id\n",
    "    }, {  # CARL SAYS HE SEES THE PILLS\n",
    "        \"chat\": 1,\n",
    "        \"turn\": 3,\n",
    "        \"author\": {\"label\": \"carl\", \"type\": [\"person\"],\n",
    "                   'uri': \"http://cltl.nl/leolani/friends/carl-1\"},\n",
    "        \"utterance\": \"Oh! Got it.\",\n",
    "        \"utterance_type\": UtteranceType.STATEMENT,\n",
    "        \"position\": \"0-25\",\n",
    "        \"subject\": {\"label\": \"carl\", \"type\": [\"person\"], 'uri': \"http://cltl.nl/leolani/world/carl-1\"},\n",
    "        \"predicate\": {\"label\": \"see\", \"uri\": \"http://cltl.nl/leolani/n2mu/see\"},\n",
    "        \"object\": {\"label\": \"pills\", \"type\": [\"object\"], 'uri': \"http://cltl.nl/leolani/world/pills\"},\n",
    "        \"perspective\": {\"certainty\": 1, \"polarity\": 1, \"sentiment\": 1},\n",
    "        \"timestamp\": datetime.combine(start_date, datetime.now().time()),\n",
    "        \"context_id\": context_id,\n",
    "    }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd40572",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def main(scenario):\n",
    "    # Create folders\n",
    "    graph_filepath = Path('./graph/carl_graph/')\n",
    "    response_filepath = Path('./responses/')\n",
    "    response_filepath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create brain connection\n",
    "    brain = LongTermMemory(address=\"http://localhost:7200/repositories/sandbox\",  # Location to save accumulated graph\n",
    "                           log_dir=graph_filepath,  # Location to save step-wise graphs\n",
    "                           clear_all=True)  # To start from an empty brain\n",
    "\n",
    "    # Loop through capsules\n",
    "    data = []\n",
    "    for (context_capsule, content_capsules) in tqdm([scenario]):\n",
    "        print(f\"\\n\\n---------------------------------------------------------------\\n\")\n",
    "\n",
    "        # Create context\n",
    "        response = brain.capsule_context(context_capsule)\n",
    "\n",
    "        # Add information to the brain\n",
    "        for capsule in content_capsules:\n",
    "            if capsule['utterance_type'] == UtteranceType.STATEMENT:\n",
    "                response = brain.capsule_statement(capsule, reason_types=True, create_label=True)\n",
    "                print(f\"\\n{capsule['triple']}\\n\")\n",
    "            if capsule['utterance_type'] == UtteranceType.EXPERIENCE:\n",
    "                response = brain.capsule_experience(capsule, create_label=True)\n",
    "                print(f\"\\n{capsule['entity']}\\n\")\n",
    "\n",
    "            response_json = brain_response_to_json(response)\n",
    "            data.append(response_json)\n",
    "\n",
    "    # Save responses \n",
    "    f = open(response_filepath / \"carl_responses.json\", \"w\")\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711b0617",
   "metadata": {},
   "outputs": [],
   "source": [
    "main(carl_scenario)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ekg-tutorial",
   "language": "python",
   "name": "ekg-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
